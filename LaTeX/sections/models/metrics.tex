\subsection{Metrics for Classification Models}

\subsubsection{Precision}
Precision measures the accuracy of positive predictions made by the model. It answers the question: ``Of all the instances predicted as positive, how many are actually positive?'' Mathematically, precision is calculated as the ratio of true positive (\(TP\)) predictions to the sum of true positive and false positive (\(FP\)) predictions:
\[
\text{Precision}:  \frac{TP}{TP + FP}
\]

\subsubsection{Recall}
Recall measures the model's ability to correctly identify all positive instances in the dataset. It answers the question: ``Of all the actual positive instances, how many did the model correctly predict as positive?'' Mathematically, recall is calculated as the ratio of true positive (\(TP\)) predictions to the sum of true positive and false negative (\(FN\)) predictions:
\[
\text{Recall}:  \frac{TP}{TP + FN}
\]

\subsubsection{F1-score}
The F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall. Mathematically, the F1-score is calculated as:
\[
\text{F1-score}: 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\]